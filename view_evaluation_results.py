"""
æŸ¥çœ‹è¯„ä¼°ç»“æœçš„ä¾¿æ·è„šæœ¬
"""

import json
import glob
from pathlib import Path
from collections import defaultdict

def view_summary():
    """æŸ¥çœ‹æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ€»ä½“ç»Ÿè®¡"""
    print("="*70)
    print("ğŸ“Š FAQè´¨é‡è¯„ä¼°ç»“æœ - æ€»ä½“ç»Ÿè®¡")
    print("="*70)
    
    files = glob.glob('output/*_evaluation.json')
    files = [f for f in files if 'analysis_report' not in f]
    
    if not files:
        print("âŒ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶ï¼")
        return
    
    results_by_doc = defaultdict(dict)
    
    for file in sorted(files):
        with open(file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è§£ææ–‡ä»¶å
        filename = Path(file).stem.replace('_evaluation', '')
        parts = filename.split('_faqs_')
        
        if len(parts) == 2:
            doc_name = parts[0]
            model = parts[1]
            
            results_by_doc[doc_name][model] = {
                'total_faqs': data['total_faqs'],
                'average_consistency': data['average_consistency'],
                'consistency_rate': data['consistency_rate'],
                'consistent_count': int(data['consistency_rate'] * data['total_faqs'])
            }
    
    # æ‰“å°è¡¨æ ¼
    print(f"\n{'æ–‡æ¡£':<20} {'æ¨¡å‹':<10} {'FAQæ•°':<10} {'å¹³å‡ä¸€è‡´æ€§':<15} {'ä¸€è‡´æ€§æ¯”ä¾‹':<15}")
    print("-"*70)
    
    total_openai = 0
    total_claude = 0
    total_consistent_openai = 0
    total_consistent_claude = 0
    
    for doc_name in sorted(results_by_doc.keys()):
        doc_display = doc_name.replace('_', ' ').title()
        
        if 'openai' in results_by_doc[doc_name]:
            r = results_by_doc[doc_name]['openai']
            total_openai += r['total_faqs']
            total_consistent_openai += r['consistent_count']
            print(f"{doc_display:<20} {'OpenAI':<10} {r['total_faqs']:<10} "
                  f"{r['average_consistency']:<15.3f} {r['consistency_rate']:<15.1%}")
        
        if 'claude' in results_by_doc[doc_name]:
            r = results_by_doc[doc_name]['claude']
            total_claude += r['total_faqs']
            total_consistent_claude += r['consistent_count']
            print(f"{doc_display:<20} {'Claude':<10} {r['total_faqs']:<10} "
                  f"{r['average_consistency']:<15.3f} {r['consistency_rate']:<15.1%}")
        print()
    
    # æ€»ä½“ç»Ÿè®¡
    print("-"*70)
    if total_openai > 0:
        avg_openai = total_consistent_openai / total_openai
        print(f"{'æ€»è®¡ (OpenAI)':<20} {'':<10} {total_openai:<10} "
              f"{'':<15} {avg_openai:.1%}")
    if total_claude > 0:
        avg_claude = total_consistent_claude / total_claude
        print(f"{'æ€»è®¡ (Claude)':<20} {'':<10} {total_claude:<10} "
              f"{'':<15} {avg_claude:.1%}")


def view_detailed(file_path, show_count=5):
    """æŸ¥çœ‹å•ä¸ªæ–‡ä»¶çš„è¯¦ç»†ç»“æœ"""
    print("="*70)
    print(f"ğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœ: {Path(file_path).name}")
    print("="*70)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»FAQæ•°: {data['total_faqs']}")
    print(f"  å¹³å‡ä¸€è‡´æ€§: {data['average_consistency']:.3f}")
    print(f"  ä¸€è‡´æ€§æ¯”ä¾‹ (>0.7): {data['consistency_rate']:.1%}")
    print(f"  ä¸€è‡´FAQæ•°: {int(data['consistency_rate'] * data['total_faqs'])}/{data['total_faqs']}")
    
    # æ˜¾ç¤ºé«˜åˆ†FAQ
    high_scores = [r for r in data['results'] if r['consistency_score'] > 0.8]
    if high_scores:
        print(f"\nâœ… é«˜åˆ†FAQç¤ºä¾‹ï¼ˆä¸€è‡´æ€§>0.8ï¼Œæ˜¾ç¤ºå‰{show_count}ä¸ªï¼‰:")
        for i, r in enumerate(high_scores[:show_count], 1):
            print(f"\n  {i}. åˆ†æ•°: {r['consistency_score']:.2f}")
            print(f"     é—®é¢˜: {r['question']}")
            print(f"     ç­”æ¡ˆ: {r['answer'][:150]}...")
    
    # æ˜¾ç¤ºä½åˆ†FAQ
    low_scores = [r for r in data['results'] if r['consistency_score'] < 0.5]
    if low_scores:
        print(f"\nâŒ ä½åˆ†FAQç¤ºä¾‹ï¼ˆä¸€è‡´æ€§<0.5ï¼Œæ˜¾ç¤ºå‰{show_count}ä¸ªï¼‰:")
        for i, r in enumerate(low_scores[:show_count], 1):
            print(f"\n  {i}. åˆ†æ•°: {r['consistency_score']:.2f}")
            print(f"     é—®é¢˜: {r['question']}")
            print(f"     ç­”æ¡ˆ: {r['answer'][:150]}...")
    
    # åˆ†æ•°åˆ†å¸ƒ
    scores = [r['consistency_score'] for r in data['results']]
    if scores:
        print(f"\nğŸ“Š åˆ†æ•°åˆ†å¸ƒ:")
        print(f"  æœ€é«˜åˆ†: {max(scores):.3f}")
        print(f"  æœ€ä½åˆ†: {min(scores):.3f}")
        print(f"  ä¸­ä½æ•°: {sorted(scores)[len(scores)//2]:.3f}")
        
        # åˆ†æ•°åŒºé—´ç»Ÿè®¡
        ranges = {
            'ä¼˜ç§€ (>0.8)': sum(1 for s in scores if s > 0.8),
            'è‰¯å¥½ (0.7-0.8)': sum(1 for s in scores if 0.7 <= s <= 0.8),
            'ä¸­ç­‰ (0.5-0.7)': sum(1 for s in scores if 0.5 <= s < 0.7),
            'è¾ƒå·® (<0.5)': sum(1 for s in scores if s < 0.5)
        }
        
        print(f"\n  åˆ†æ•°åŒºé—´åˆ†å¸ƒ:")
        for range_name, count in ranges.items():
            pct = count / len(scores) * 100
            print(f"    {range_name}: {count} ({pct:.1f}%)")


def view_comparison():
    """å¯¹æ¯”OpenAIå’ŒClaudeçš„ç»“æœ"""
    print("="*70)
    print("ğŸ“Š OpenAI vs Claude å¯¹æ¯”åˆ†æ")
    print("="*70)
    
    files = glob.glob('output/*_evaluation.json')
    files = [f for f in files if 'analysis_report' not in f]
    
    results_by_doc = defaultdict(dict)
    
    for file in sorted(files):
        with open(file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        filename = Path(file).stem.replace('_evaluation', '')
        parts = filename.split('_faqs_')
        
        if len(parts) == 2:
            doc_name = parts[0]
            model = parts[1]
            results_by_doc[doc_name][model] = data
    
    for doc_name in sorted(results_by_doc.keys()):
        if 'openai' in results_by_doc[doc_name] and 'claude' in results_by_doc[doc_name]:
            doc_display = doc_name.replace('_', ' ').title()
            openai_data = results_by_doc[doc_name]['openai']
            claude_data = results_by_doc[doc_name]['claude']
            
            print(f"\n{doc_display}:")
            print(f"  OpenAI: å¹³å‡ä¸€è‡´æ€§={openai_data['average_consistency']:.3f}, "
                  f"ä¸€è‡´æ€§æ¯”ä¾‹={openai_data['consistency_rate']:.1%}")
            print(f"  Claude: å¹³å‡ä¸€è‡´æ€§={claude_data['average_consistency']:.3f}, "
                  f"ä¸€è‡´æ€§æ¯”ä¾‹={claude_data['consistency_rate']:.1%}")
            
            diff = openai_data['average_consistency'] - claude_data['average_consistency']
            if abs(diff) > 0.05:
                winner = "OpenAI" if diff > 0 else "Claude"
                print(f"  â†’ {winner}è¡¨ç°æ›´å¥½ï¼ˆå·®å¼‚: {abs(diff):.3f}ï¼‰")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "summary":
            view_summary()
        elif sys.argv[1] == "comparison":
            view_comparison()
        elif sys.argv[1].startswith("output/"):
            view_detailed(sys.argv[1])
        else:
            print("ç”¨æ³•:")
            print("  python view_evaluation_results.py summary      # æŸ¥çœ‹æ€»ä½“ç»Ÿè®¡")
            print("  python view_evaluation_results.py comparison   # æŸ¥çœ‹å¯¹æ¯”åˆ†æ")
            print("  python view_evaluation_results.py output/xxx_evaluation.json  # æŸ¥çœ‹è¯¦ç»†ç»“æœ")
    else:
        # é»˜è®¤æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        view_summary()
        print("\n" + "="*70)
        print("ğŸ’¡ æ›´å¤šé€‰é¡¹:")
        print("  python view_evaluation_results.py summary      # æŸ¥çœ‹æ€»ä½“ç»Ÿè®¡")
        print("  python view_evaluation_results.py comparison   # æŸ¥çœ‹å¯¹æ¯”åˆ†æ")
        print("  python view_evaluation_results.py output/genai_faqs_openai_evaluation.json  # æŸ¥çœ‹è¯¦ç»†ç»“æœ")


if __name__ == "__main__":
    main()

