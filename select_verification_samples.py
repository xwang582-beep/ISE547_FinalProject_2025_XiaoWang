#!/usr/bin/env python3
"""
é€‰æ‹©æ‰‹åŠ¨éªŒè¯çš„FAQæ ·æœ¬
ä»æ¯ä¸ªæ–‡æ¡£ä¸­é€‰æ‹©ä¸åŒè´¨é‡æ°´å¹³çš„FAQï¼Œç”Ÿæˆä¾¿äºéªŒè¯çš„æ ¼å¼
"""

import json
import random
from pathlib import Path
from collections import defaultdict

def load_faqs_from_evaluation(eval_file):
    """ä»è¯„ä¼°ç»“æœæ–‡ä»¶ä¸­åŠ è½½FAQ"""
    with open(eval_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ä»resultsä¸­æå–FAQ
    faqs = []
    for result in data.get('results', []):
        faq = {
            'question': result.get('question', ''),
            'answer': result.get('answer', ''),
            'consistency_score': result.get('consistency_score', 0),
            'is_consistent': result.get('is_consistent', False),
            'faq_index': result.get('faq_index', 0)
        }
        faqs.append(faq)
    
    return faqs, data.get('file', ''), data.get('method', '')

def load_faqs_from_original(original_file):
    """ä»åŸå§‹FAQæ–‡ä»¶ä¸­åŠ è½½å®Œæ•´FAQä¿¡æ¯"""
    try:
        with open(original_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data.get('faqs', [])
    except:
        return []

def categorize_faqs(faqs):
    """å°†FAQæŒ‰åˆ†æ•°åˆ†ç±»"""
    high = []  # >= 0.7
    medium = []  # 0.5 - 0.7
    low = []  # < 0.5
    
    for faq in faqs:
        score = faq.get('consistency_score', 0)
        if score >= 0.7:
            high.append(faq)
        elif score >= 0.5:
            medium.append(faq)
        else:
            low.append(faq)
    
    return high, medium, low

def select_samples(high, medium, low, total_samples=20):
    """ä»ä¸åŒç±»åˆ«ä¸­é€‰æ‹©æ ·æœ¬"""
    # åˆ†é…æ ·æœ¬æ•°é‡ï¼š40%é«˜åˆ†ï¼Œ30%ä¸­åˆ†ï¼Œ30%ä½åˆ†
    high_count = max(1, int(total_samples * 0.4))
    medium_count = max(1, int(total_samples * 0.3))
    low_count = max(1, total_samples - high_count - medium_count)
    
    # å¦‚æœæŸä¸ªç±»åˆ«æ•°é‡ä¸è¶³ï¼Œä»å…¶ä»–ç±»åˆ«è¡¥å……
    selected = []
    
    # é€‰æ‹©é«˜åˆ†
    if high:
        high_samples = random.sample(high, min(high_count, len(high)))
        selected.extend(high_samples)
        remaining = total_samples - len(selected)
    else:
        remaining = total_samples
    
    # é€‰æ‹©ä¸­åˆ†
    if medium and remaining > 0:
        medium_samples = random.sample(medium, min(medium_count, len(medium), remaining))
        selected.extend(medium_samples)
        remaining = total_samples - len(selected)
    
    # é€‰æ‹©ä½åˆ†
    if low and remaining > 0:
        low_samples = random.sample(low, min(low_count, len(low), remaining))
        selected.extend(low_samples)
    
    # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»æ‰€æœ‰ç±»åˆ«ä¸­éšæœºè¡¥å……
    all_faqs = high + medium + low
    while len(selected) < total_samples and len(selected) < len(all_faqs):
        remaining_faqs = [f for f in all_faqs if f not in selected]
        if not remaining_faqs:
            break
        selected.append(random.choice(remaining_faqs))
    
    return selected

def generate_verification_document(samples_by_file):
    """ç”Ÿæˆæ‰‹åŠ¨éªŒè¯æ–‡æ¡£"""
    markdown = """# ğŸ“‹ FAQæ‰‹åŠ¨éªŒè¯æ ·æœ¬

## ğŸ“Š éªŒè¯è¯´æ˜

æœ¬æ–‡æ¡£åŒ…å«ä»æ‰€æœ‰æ–‡æ¡£ä¸­éšæœºé€‰æ‹©çš„FAQæ ·æœ¬ï¼Œç”¨äºæ‰‹åŠ¨éªŒè¯è‡ªåŠ¨åŒ–è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

### éªŒè¯æ ‡å‡†

1. **å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰**
   - âœ… **å‡†ç¡®**ï¼šç­”æ¡ˆå®Œå…¨åŸºäºæºæ–‡æ¡£ï¼Œä¿¡æ¯æ­£ç¡®
   - âš ï¸ **éƒ¨åˆ†å‡†ç¡®**ï¼šç­”æ¡ˆåŸºæœ¬æ­£ç¡®ï¼Œä½†ç¼ºå°‘ç»†èŠ‚æˆ–ç•¥æœ‰åå·®
   - âŒ **ä¸å‡†ç¡®**ï¼šç­”æ¡ˆåŒ…å«é”™è¯¯ä¿¡æ¯æˆ–ä¸æºæ–‡æ¡£ä¸ç¬¦

2. **ç›¸å…³æ€§ï¼ˆRelevanceï¼‰**
   - âœ… **é«˜åº¦ç›¸å…³**ï¼šé—®é¢˜ç›´æ¥å¯¹åº”æ–‡æ¡£ä¸­çš„æ˜ç¡®ä¿¡æ¯
   - âš ï¸ **éƒ¨åˆ†ç›¸å…³**ï¼šé—®é¢˜ç›¸å…³ä½†ç­”æ¡ˆä¸å¤Ÿå…·ä½“
   - âŒ **ä¸ç›¸å…³**ï¼šé—®é¢˜ä¸æ–‡æ¡£å†…å®¹å…³ç³»ä¸å¤§

3. **è‡ªç„¶åº¦ï¼ˆNaturalnessï¼‰**
   - âœ… **è‡ªç„¶**ï¼šé—®é¢˜åƒäººç±»ä¼šé—®çš„é—®é¢˜ï¼Œç­”æ¡ˆæµç•…
   - âš ï¸ **ä¸€èˆ¬**ï¼šé—®é¢˜æˆ–ç­”æ¡ˆç•¥æ˜¾ç”Ÿç¡¬
   - âŒ **ä¸è‡ªç„¶**ï¼šé—®é¢˜æˆ–ç­”æ¡ˆæ˜æ˜¾æ˜¯æœºå™¨ç”Ÿæˆçš„

4. **å®Œæ•´æ€§ï¼ˆCompletenessï¼‰**
   - âœ… **å®Œæ•´**ï¼šç­”æ¡ˆå……åˆ†å›ç­”äº†é—®é¢˜
   - âš ï¸ **éƒ¨åˆ†å®Œæ•´**ï¼šç­”æ¡ˆå›ç­”äº†é—®é¢˜ä½†ä¸å¤Ÿè¯¦ç»†
   - âŒ **ä¸å®Œæ•´**ï¼šç­”æ¡ˆæ²¡æœ‰å……åˆ†å›ç­”é—®é¢˜

### éªŒè¯æ–¹æ³•

1. é˜…è¯»æ¯ä¸ªFAQçš„é—®é¢˜å’Œç­”æ¡ˆ
2. åœ¨æºæ–‡æ¡£ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯
3. è¯„ä¼°å‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€è‡ªç„¶åº¦ã€å®Œæ•´æ€§
4. è®°å½•ä¸è‡ªåŠ¨åŒ–è¯„ä¼°çš„ä¸€è‡´æ€§åˆ†æ•°æ˜¯å¦åŒ¹é…
5. ç‰¹åˆ«å…³æ³¨ä½åˆ†FAQï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨è¡¨è¾¾åå·®ï¼ˆç­”æ¡ˆæ­£ç¡®ä½†è¡¨è¾¾ä¸åŒï¼‰

---

"""
    
    total_faqs = 0
    
    for filename, samples in samples_by_file.items():
        doc_name = filename.replace('_evaluation', '').replace('_faqs', '').replace('_', ' ').title()
        method = samples[0].get('method', 'Unknown')
        
        markdown += f"## ğŸ“„ {doc_name}\n\n"
        markdown += f"**è¯„ä¼°æ–¹æ³•**: {method.upper()}\n\n"
        markdown += f"**æ ·æœ¬æ•°é‡**: {len(samples)} ä¸ªFAQ\n\n"
        markdown += "---\n\n"
        
        for i, sample in enumerate(samples, 1):
            question = sample.get('question', 'N/A')
            answer = sample.get('answer', 'N/A')
            score = sample.get('consistency_score', 0)
            is_consistent = sample.get('is_consistent', False)
            quality_level = "é«˜åˆ†" if score >= 0.7 else ("ä¸­åˆ†" if score >= 0.5 else "ä½åˆ†")
            
            markdown += f"### FAQ #{i} - {quality_level} (ä¸€è‡´æ€§åˆ†æ•°: {score:.3f})\n\n"
            markdown += f"**é—®é¢˜**:\n```\n{question}\n```\n\n"
            markdown += f"**ç­”æ¡ˆ**:\n```\n{answer}\n```\n\n"
            markdown += f"**è‡ªåŠ¨åŒ–è¯„ä¼°**:\n"
            markdown += f"- ä¸€è‡´æ€§åˆ†æ•°: **{score:.3f}**\n"
            markdown += f"- æ˜¯å¦ä¸€è‡´ (>0.7): {'âœ… æ˜¯' if is_consistent else 'âŒ å¦'}\n"
            markdown += f"- è´¨é‡ç­‰çº§: **{quality_level}**\n\n"
            markdown += "**äººå·¥è¯„ä¼°**:\n"
            markdown += "- å‡†ç¡®æ€§: [ ] âœ… å‡†ç¡®  [ ] âš ï¸ éƒ¨åˆ†å‡†ç¡®  [ ] âŒ ä¸å‡†ç¡®\n"
            markdown += "- ç›¸å…³æ€§: [ ] âœ… é«˜åº¦ç›¸å…³  [ ] âš ï¸ éƒ¨åˆ†ç›¸å…³  [ ] âŒ ä¸ç›¸å…³\n"
            markdown += "- è‡ªç„¶åº¦: [ ] âœ… è‡ªç„¶  [ ] âš ï¸ ä¸€èˆ¬  [ ] âŒ ä¸è‡ªç„¶\n"
            markdown += "- å®Œæ•´æ€§: [ ] âœ… å®Œæ•´  [ ] âš ï¸ éƒ¨åˆ†å®Œæ•´  [ ] âŒ ä¸å®Œæ•´\n\n"
            markdown += "**åœ¨æºæ–‡æ¡£ä¸­çš„ä½ç½®**:\n"
            markdown += "- ç« èŠ‚/é¡µç : _______________\n\n"
            markdown += "**è§‚å¯Ÿ**:\n"
            markdown += "- [ ] ç­”æ¡ˆåŒ…å«æºæ–‡æ¡£æ²¡æœ‰çš„ä¿¡æ¯\n"
            markdown += "- [ ] ç­”æ¡ˆè¿‡äºæ¦‚æ‹¬ï¼Œç¼ºå°‘ç»†èŠ‚\n"
            markdown += "- [ ] ç­”æ¡ˆè¡¨è¾¾æ–¹å¼ä¸åŒä½†æ„æ€ç›¸åŒï¼ˆå¯èƒ½æ˜¯è¯„ä¼°åå·®ï¼‰\n"
            markdown += "- [ ] å…¶ä»–é—®é¢˜: _______________\n\n"
            markdown += "**å¤‡æ³¨**:\n```\n\n```\n\n"
            markdown += "---\n\n"
        
        total_faqs += len(samples)
    
    markdown += f"\n## ğŸ“Š ç»Ÿè®¡æ€»ç»“\n\n"
    markdown += f"**æ€»éªŒè¯æ ·æœ¬æ•°**: {total_faqs} ä¸ªFAQ\n\n"
    markdown += "### éªŒè¯å®Œæˆåï¼Œè¯·å¡«å†™ä»¥ä¸‹ç»Ÿè®¡ï¼š\n\n"
    markdown += "**å‡†ç¡®æ€§åˆ†å¸ƒ**:\n"
    markdown += "- âœ… å‡†ç¡®: ____ (____%)\n"
    markdown += "- âš ï¸ éƒ¨åˆ†å‡†ç¡®: ____ (____%)\n"
    markdown += "- âŒ ä¸å‡†ç¡®: ____ (____%)\n\n"
    markdown += "**ä¸è‡ªåŠ¨åŒ–è¯„ä¼°çš„ä¸€è‡´æ€§**:\n"
    markdown += "- é«˜ä¸€è‡´æ€§åˆ†æ•°ï¼ˆ>0.7ï¼‰çš„FAQä¸­ï¼Œäººå·¥è¯„ä¼°ä¸ºå‡†ç¡®çš„æ¯”ä¾‹: ____ / ____ (____%)\n"
    markdown += "- ä½ä¸€è‡´æ€§åˆ†æ•°ï¼ˆ<0.5ï¼‰çš„FAQä¸­ï¼Œäººå·¥è¯„ä¼°ä¸ºå‡†ç¡®çš„æ¯”ä¾‹: ____ / ____ (____%)\n"
    markdown += "- æ€»ä½“ä¸€è‡´æ€§: ____%\n\n"
    markdown += "**ä¸»è¦å‘ç°**:\n```\n\n```\n\n"
    
    return markdown

def main():
    """ä¸»å‡½æ•°"""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='é€‰æ‹©æ‰‹åŠ¨éªŒè¯çš„FAQæ ·æœ¬')
    parser.add_argument('samples', type=int, nargs='?', default=20,
                       help='æ¯ä¸ªæ–‡æ¡£é€‰æ‹©çš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰')
    parser.add_argument('--low-score-only', action='store_true',
                       help='åªé€‰æ‹©ä½åˆ†FAQï¼ˆ<0.5ï¼‰')
    parser.add_argument('--high-score-only', action='store_true',
                       help='åªé€‰æ‹©é«˜åˆ†FAQï¼ˆâ‰¥0.7ï¼‰')
    
    args = parser.parse_args()
    samples_per_file = args.samples
    low_score_only = args.low_score_only
    high_score_only = args.high_score_only
    
    print("=" * 80)
    print("ğŸ“‹ é€‰æ‹©æ‰‹åŠ¨éªŒè¯æ ·æœ¬")
    print("=" * 80)
    print(f"\næ¯ä¸ªæ–‡æ¡£é€‰æ‹©æ ·æœ¬æ•°: {samples_per_file}")
    if low_score_only:
        print("æ¨¡å¼: åªé€‰æ‹©ä½åˆ†FAQï¼ˆ<0.5ï¼‰")
    elif high_score_only:
        print("æ¨¡å¼: åªé€‰æ‹©é«˜åˆ†FAQï¼ˆâ‰¥0.7ï¼‰")
    else:
        print("æ¨¡å¼: è¦†ç›–ä¸åŒè´¨é‡æ°´å¹³ï¼ˆé«˜åˆ†40%ï¼Œä¸­åˆ†30%ï¼Œä½åˆ†30%ï¼‰")
    print()
    
    # æŸ¥æ‰¾æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
    output_dir = Path("output")
    eval_files = list(output_dir.glob("*_evaluation.json"))
    eval_files = [f for f in eval_files if 'analysis_report' not in f.name]
    
    if not eval_files:
        print("âŒ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶ï¼")
        print("   è¯·å…ˆè¿è¡Œè¯„ä¼°è„šæœ¬ï¼špython evaluate_with_course_methods.py")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(eval_files)} ä¸ªè¯„ä¼°ç»“æœæ–‡ä»¶")
    print()
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    all_samples = {}
    
    for eval_file in sorted(eval_files):
        print(f"å¤„ç†: {eval_file.name}")
        
        # åŠ è½½FAQ
        faqs, original_file, method = load_faqs_from_evaluation(eval_file)
        
        if not faqs:
            print(f"  âš ï¸  æœªæ‰¾åˆ°FAQæ•°æ®ï¼Œè·³è¿‡")
            continue
        
        print(f"  æ€»FAQæ•°: {len(faqs)}")
        
        # åˆ†ç±»
        high, medium, low = categorize_faqs(faqs)
        print(f"  é«˜åˆ† (â‰¥0.7): {len(high)}, ä¸­åˆ† (0.5-0.7): {len(medium)}, ä½åˆ† (<0.5): {len(low)}")
        
        # é€‰æ‹©æ ·æœ¬
        if low_score_only:
            # åªé€‰æ‹©ä½åˆ†FAQ
            if low:
                selected = random.sample(low, min(samples_per_file, len(low)))
            else:
                print(f"  âš ï¸  æ²¡æœ‰ä½åˆ†FAQï¼Œè·³è¿‡")
                continue
        elif high_score_only:
            # åªé€‰æ‹©é«˜åˆ†FAQ
            if high:
                selected = random.sample(high, min(samples_per_file, len(high)))
            else:
                print(f"  âš ï¸  æ²¡æœ‰é«˜åˆ†FAQï¼Œè·³è¿‡")
                continue
        else:
            # æ­£å¸¸é€‰æ‹©ï¼ˆè¦†ç›–ä¸åŒè´¨é‡æ°´å¹³ï¼‰
            selected = select_samples(high, medium, low, samples_per_file)
        
        # æ·»åŠ æ–¹æ³•ä¿¡æ¯
        for s in selected:
            s['method'] = method
        
        all_samples[eval_file.stem] = selected
        print(f"  âœ… é€‰æ‹©äº† {len(selected)} ä¸ªæ ·æœ¬")
        print()
    
    # ç”ŸæˆéªŒè¯æ–‡æ¡£
    print("ç”ŸæˆéªŒè¯æ–‡æ¡£...")
    verification_doc = generate_verification_document(all_samples)
    
    # ä¿å­˜
    output_file = "manual_verification_samples.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(verification_doc)
    
    print(f"âœ… éªŒè¯æ–‡æ¡£å·²ä¿å­˜åˆ°: {output_file}")
    print()
    print("=" * 80)
    print("ğŸ“ ä¸‹ä¸€æ­¥:")
    print("=" * 80)
    print(f"1. æ‰“å¼€ {output_file} æ–‡ä»¶")
    print("2. é€ä¸ªéªŒè¯æ¯ä¸ªFAQ")
    print("3. å¡«å†™è¯„ä¼°ç»“æœ")
    print("4. å®Œæˆåç»Ÿè®¡ç»“æœï¼Œä¸è‡ªåŠ¨åŒ–è¯„ä¼°å¯¹æ¯”")
    print()
    print("ğŸ’¡ æç¤º:")
    print("- ç‰¹åˆ«å…³æ³¨ä½åˆ†FAQï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨è¡¨è¾¾åå·®")
    print("- è®°å½•åœ¨æºæ–‡æ¡£ä¸­çš„ä½ç½®ï¼Œä¾¿äºéªŒè¯å‡†ç¡®æ€§")
    print("- å¦‚æœå‘ç°è‡ªåŠ¨åŒ–è¯„ä¼°ä¸äººå·¥è¯„ä¼°å·®å¼‚è¾ƒå¤§ï¼Œè®°å½•ä¸‹æ¥")

if __name__ == "__main__":
    main()

