"""
ä½¿ç”¨è¯¾ç¨‹æ–¹æ³•è¯„ä¼°FAQè´¨é‡
åŸºäºQAFactEvalå’ŒQuestEvalæ–¹æ³•
"""

import json
import glob
from pathlib import Path
from typing import List, Dict
import os

try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from anthropic import Anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False


class FAQEvaluator:
    """ä½¿ç”¨è¯¾ç¨‹æ–¹æ³•è¯„ä¼°FAQè´¨é‡"""
    
    def __init__(self, api_key: str = None, provider: str = "openai"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            api_key: APIå¯†é’¥
            provider: LLMæä¾›å•†ï¼ˆopenaiæˆ–anthropicï¼‰
        """
        self.provider = provider.lower()
        
        if self.provider == "openai" and HAS_OPENAI:
            self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
            self.model = "gpt-3.5-turbo"
        elif self.provider == "anthropic" and HAS_ANTHROPIC:
            self.client = Anthropic(api_key=api_key or os.getenv('ANTHROPIC_API_KEY'))
            self.model = "claude-3-haiku-20240307"
        else:
            self.client = None
            print("âš ï¸  LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨ç®€åŒ–è¯„ä¼°æ–¹æ³•")
    
    def qafacteval_method(self, faq: Dict, source_chunk: str) -> Dict:
        """
        QAFactEvalæ–¹æ³•ï¼šä»FAQç­”æ¡ˆç”Ÿæˆé—®é¢˜ï¼Œåœ¨æºæ–‡æ¡£ä¸­æŸ¥æ‰¾ç­”æ¡ˆï¼Œè¯„ä¼°ä¸€è‡´æ€§
        
        Args:
            faq: FAQå­—å…¸ï¼ŒåŒ…å«'question'å’Œ'answer'
            source_chunk: æºæ–‡æ¡£çš„ç›¸å…³æ–‡æœ¬å—
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.client:
            return self._simple_consistency_check(faq, source_chunk)
        
        # æ­¥éª¤1: ä»FAQç­”æ¡ˆç”Ÿæˆé—®é¢˜ï¼ˆä½¿ç”¨LLMï¼‰
        questions = self._generate_questions_from_answer(faq['answer'])
        
        # æ­¥éª¤2: åœ¨æºæ–‡æ¡£ä¸­æŸ¥æ‰¾è¿™äº›é—®é¢˜çš„ç­”æ¡ˆ
        source_answers = []
        for question in questions:
            answer = self._find_answer_in_source(question, source_chunk)
            source_answers.append(answer)
        
        # æ­¥éª¤3: è¯„ä¼°ä¸€è‡´æ€§
        consistency_score = self._calculate_consistency(
            faq['answer'],
            source_answers,
            questions
        )
        
        return {
            'method': 'QAFactEval',
            'consistency_score': consistency_score,
            'generated_questions': questions,
            'source_answers': source_answers,
            'is_consistent': consistency_score >= 0.7  # é˜ˆå€¼0.7
        }
    
    def questeval_method(self, faq: Dict, source_chunk: str) -> Dict:
        """
        QuestEvalæ–¹æ³•ï¼šä»FAQç­”æ¡ˆç”Ÿæˆé—®é¢˜ï¼Œåœ¨æºæ–‡æ¡£å’ŒFAQç­”æ¡ˆä¸­æ‰¾ç­”æ¡ˆï¼Œæ¯”è¾ƒä¸€è‡´æ€§
        
        Args:
            faq: FAQå­—å…¸
            source_chunk: æºæ–‡æ¡£æ–‡æœ¬å—
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.client:
            return self._simple_consistency_check(faq, source_chunk)
        
        # æ­¥éª¤1: ä»FAQç­”æ¡ˆç”Ÿæˆé—®é¢˜
        questions = self._generate_questions_from_answer(faq['answer'])
        
        # æ­¥éª¤2: åœ¨æºæ–‡æ¡£ä¸­æŸ¥æ‰¾ç­”æ¡ˆ
        source_answers = [self._find_answer_in_source(q, source_chunk) 
                         for q in questions]
        
        # æ­¥éª¤3: åœ¨FAQç­”æ¡ˆä¸­æŸ¥æ‰¾ç­”æ¡ˆï¼ˆè‡ªéªŒè¯ï¼‰
        faq_answers = [self._extract_answer_from_faq(q, faq['answer']) 
                      for q in questions]
        
        # æ­¥éª¤4: è®¡ç®—ä¸€è‡´æ€§
        consistency = self._compare_answers(source_answers, faq_answers)
        
        return {
            'method': 'QuestEval',
            'consistency_score': consistency,
            'generated_questions': questions,
            'source_answers': source_answers,
            'faq_answers': faq_answers,
            'is_consistent': consistency >= 0.7
        }
    
    def _generate_questions_from_answer(self, answer: str) -> List[str]:
        """ä»ç­”æ¡ˆç”Ÿæˆé—®é¢˜"""
        if not self.client:
            # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›ç©ºåˆ—è¡¨
            return []
        
        prompt = f"""
        Generate 2-3 questions that could be answered by the following text.
        Return only the questions, one per line.
        
        Text:
        {answer}
        """
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=100
                )
                text = response.choices[0].message.content
            else:  # anthropic
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=100,
                    messages=[{"role": "user", "content": prompt}]
                )
                text = response.content[0].text
            
            # è§£æé—®é¢˜
            questions = [q.strip() for q in text.split('\n') if q.strip() and '?' in q]
            return questions[:3]  # æœ€å¤š3ä¸ªé—®é¢˜
            
        except Exception as e:
            print(f"ç”Ÿæˆé—®é¢˜å¤±è´¥: {str(e)}")
            return []
    
    def _find_answer_in_source(self, question: str, source: str) -> str:
        """åœ¨æºæ–‡æ¡£ä¸­æŸ¥æ‰¾é—®é¢˜çš„ç­”æ¡ˆ"""
        if not self.client:
            return ""
        
        prompt = f"""
        Based on the following source text, answer this question briefly (1-2 sentences):
        
        Question: {question}
        
        Source Text:
        {source[:1000]}  # é™åˆ¶é•¿åº¦
        
        Answer:
        """
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=100
                )
                return response.choices[0].message.content.strip()
            else:
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=100,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text.strip()
        except Exception as e:
            return ""
    
    def _extract_answer_from_faq(self, question: str, faq_answer: str) -> str:
        """ä»FAQç­”æ¡ˆä¸­æå–å¯¹é—®é¢˜çš„å›ç­”"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå¦‚æœé—®é¢˜å…³é”®è¯åœ¨ç­”æ¡ˆä¸­ï¼Œè¿”å›ç›¸å…³éƒ¨åˆ†
        question_words = set(question.lower().split())
        answer_sentences = faq_answer.split('.')
        
        for sentence in answer_sentences:
            sentence_words = set(sentence.lower().split())
            if len(question_words.intersection(sentence_words)) >= 2:
                return sentence.strip()
        
        return faq_answer[:100]  # è¿”å›å‰100å­—ç¬¦
    
    def _calculate_consistency(self, faq_answer: str, source_answers: List[str], 
                              questions: List[str]) -> float:
        """è®¡ç®—FAQç­”æ¡ˆä¸æºæ–‡æ¡£ç­”æ¡ˆçš„ä¸€è‡´æ€§"""
        if not self.client or not source_answers:
            return self._simple_similarity(faq_answer, ' '.join(source_answers))
        
        # ä½¿ç”¨LLMè¯„ä¼°ä¸€è‡´æ€§
        source_text = ' '.join([f"Q: {q}\nA: {a}" for q, a in zip(questions, source_answers)])
        
        prompt = f"""
        Rate the consistency between these two answers (0-10):
        
        FAQ Answer:
        {faq_answer}
        
        Source-based Answers:
        {source_text}
        
        Score (0-10, where 10 is completely consistent):
        """
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=50
                )
                text = response.choices[0].message.content
            else:
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=50,
                    messages=[{"role": "user", "content": prompt}]
                )
                text = response.content[0].text
            
            # æå–åˆ†æ•°
            import re
            score_match = re.search(r'(\d+(?:\.\d+)?)', text)
            if score_match:
                score = float(score_match.group(1))
                return min(score / 10.0, 1.0)  # è½¬æ¢ä¸º0-1èŒƒå›´
            
        except Exception as e:
            print(f"è®¡ç®—ä¸€è‡´æ€§å¤±è´¥: {str(e)}")
        
        return 0.5  # é»˜è®¤åˆ†æ•°
    
    def _compare_answers(self, source_answers: List[str], faq_answers: List[str]) -> float:
        """æ¯”è¾ƒæºæ–‡æ¡£ç­”æ¡ˆå’ŒFAQç­”æ¡ˆ"""
        if not source_answers or not faq_answers:
            return 0.0
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        similarities = []
        for s_ans, f_ans in zip(source_answers, faq_answers):
            sim = self._simple_similarity(s_ans, f_ans)
            similarities.append(sim)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„è¯é‡å ç›¸ä¼¼åº¦"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _simple_consistency_check(self, faq: Dict, source_chunk: str) -> Dict:
        """ç®€åŒ–çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆä¸ä½¿ç”¨LLMï¼‰"""
        # åŸºäºå…³é”®è¯é‡å 
        answer_words = set(faq['answer'].lower().split())
        source_words = set(source_chunk.lower().split())
        
        overlap = answer_words.intersection(source_words)
        similarity = len(overlap) / len(answer_words) if answer_words else 0.0
        
        return {
            'method': 'Simple',
            'consistency_score': similarity,
            'is_consistent': similarity >= 0.3
        }


def evaluate_faqs_from_json(json_file: str, source_text: str = None, 
                           method: str = "qafacteval", sample_size: int = None):
    """
    ä»JSONæ–‡ä»¶è¯„ä¼°FAQè´¨é‡
    
    Args:
        json_file: FAQ JSONæ–‡ä»¶è·¯å¾„
        method: è¯„ä¼°æ–¹æ³•ï¼ˆqafactevalæˆ–questevalï¼‰
        sample_size: è¯„ä¼°çš„FAQæ•°é‡ï¼ˆå¦‚æœä¸ºNoneæˆ–0ï¼Œè¯„ä¼°æ‰€æœ‰ï¼‰
    """
    print(f"\n{'='*70}")
    print(f"è¯„ä¼°æ–‡ä»¶: {json_file}")
    print(f"æ–¹æ³•: {method}")
    print(f"{'='*70}")
    
    # è¯»å–FAQæ•°æ®
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    faqs = data.get('faqs', [])
    print(f"æ€»FAQæ•°: {len(faqs)}")
    
    # æŠ½æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if sample_size and sample_size > 0 and sample_size < len(faqs):
        import random
        faqs = random.sample(faqs, sample_size)
        print(f"æŠ½æ ·è¯„ä¼°: {len(faqs)} ä¸ªFAQ")
    else:
        print(f"å…¨æ ·æœ¬è¯„ä¼°: {len(faqs)} ä¸ªFAQ")
    
    # å‡†å¤‡æºæ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰
    source_chunks = None
    if source_text:
        # ç®€å•åˆ†å—
        chunk_size = 500
        source_chunks = [source_text[i:i+chunk_size] 
                        for i in range(0, len(source_text), chunk_size)]
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = FAQEvaluator()
    
    # è¯„ä¼°æ¯ä¸ªFAQ
    results = []
    consistent_count = 0
    
    print(f"\nå¼€å§‹è¯„ä¼° {len(faqs)} ä¸ªFAQ...")
    print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")
    
    for i, faq in enumerate(faqs):
        if (i + 1) % 10 == 0 or (i + 1) == len(faqs):
            print(f"  å¤„ç†ä¸­: {i+1}/{len(faqs)} ({((i+1)/len(faqs)*100):.1f}%)")
        
        # é€‰æ‹©æºæ–‡æ¡£å—ï¼ˆå¦‚æœæœ‰ï¼‰
        source_chunk = ""
        if source_chunks:
            # ä½¿ç”¨FAQçš„chunk_idæ‰¾åˆ°å¯¹åº”çš„æºæ–‡æ¡£å—
            chunk_id = faq.get('chunk_id', 0)
            if chunk_id < len(source_chunks):
                source_chunk = source_chunks[chunk_id]
            else:
                source_chunk = source_chunks[0]  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª
        elif source_text:
            source_chunk = source_text[:1000]  # ä½¿ç”¨å‰1000å­—ç¬¦
        
        # è¯„ä¼°
        if method == "qafacteval":
            result = evaluator.qafacteval_method(faq, source_chunk)
        else:
            result = evaluator.questeval_method(faq, source_chunk)
        
        result['faq_index'] = i
        result['question'] = faq.get('question', '')[:80]
        result['answer'] = faq.get('answer', '')[:100]
        
        results.append(result)
        
        if result.get('is_consistent', False):
            consistent_count += 1
    
    # ç»Ÿè®¡
    consistency_scores = [r['consistency_score'] for r in results]
    avg_score = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0
    consistency_rate = consistent_count / len(faqs) if faqs else 0
    
    print(f"\nè¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡ä¸€è‡´æ€§åˆ†æ•°: {avg_score:.3f}")
    print(f"  ä¸€è‡´æ€§æ¯”ä¾‹ (>0.7): {consistency_rate:.1%}")
    print(f"  ä¸€è‡´FAQæ•°: {consistent_count}/{len(faqs)}")
    
    # ä¿å­˜ç»“æœ
    output_file = json_file.replace('.json', '_evaluation.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'file': json_file,
            'method': method,
            'total_faqs': len(faqs),
            'average_consistency': avg_score,
            'consistency_rate': consistency_rate,
            'results': results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return {
        'average_consistency': avg_score,
        'consistency_rate': consistency_rate,
        'consistent_count': consistent_count,
        'total_count': len(faqs)
    }


def main():
    """ä¸»å‡½æ•°ï¼šè¯„ä¼°æ‰€æœ‰JSONæ–‡ä»¶"""
    import sys
    
    print("="*70)
    print("ğŸ“Š ä½¿ç”¨è¯¾ç¨‹æ–¹æ³•è¯„ä¼°FAQè´¨é‡")
    print("="*70)
    print("\næ–¹æ³•è¯´æ˜:")
    print("1. QAFactEval: ä»FAQç­”æ¡ˆç”Ÿæˆé—®é¢˜ï¼Œåœ¨æºæ–‡æ¡£ä¸­æ‰¾ç­”æ¡ˆï¼Œè¯„ä¼°ä¸€è‡´æ€§")
    print("2. QuestEval: åŒé‡éªŒè¯ï¼Œæ¯”è¾ƒæºæ–‡æ¡£å’ŒFAQç­”æ¡ˆçš„ä¸€è‡´æ€§")
    print("\næ³¨æ„: å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•")
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = glob.glob('output/*.json')
    json_files = [f for f in json_files if '_evaluation' not in f]  # æ’é™¤è¯„ä¼°ç»“æœæ–‡ä»¶
    
    if not json_files:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶ï¼")
        return
    
    print(f"\næ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ–¹æ³•ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
    if len(sys.argv) > 1:
        method = sys.argv[1].lower()
        if method not in ['qafacteval', 'questeval']:
            print(f"âš ï¸  æ— æ•ˆçš„æ–¹æ³•: {method}ï¼Œä½¿ç”¨é»˜è®¤å€¼: qafacteval")
            method = "qafacteval"
    else:
        method = "qafacteval"
        print(f"\nä½¿ç”¨é»˜è®¤æ–¹æ³•: {method}")
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ ·æœ¬å¤§å°ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
    if len(sys.argv) > 2:
        try:
            sample_size_input = sys.argv[2].lower()
            if sample_size_input in ['all', 'full', 'none', '0']:
                sample_size = None  # è¯„ä¼°æ‰€æœ‰
                print(f"ä½¿ç”¨å…¨æ ·æœ¬è¯„ä¼°ï¼ˆæ‰€æœ‰FAQï¼‰")
            else:
                sample_size = int(sample_size_input)
                print(f"ä½¿ç”¨æ ·æœ¬å¤§å°: {sample_size}")
        except ValueError:
            sample_size = None
            print(f"âš ï¸  æ— æ•ˆçš„æ ·æœ¬å¤§å°ï¼Œä½¿ç”¨å…¨æ ·æœ¬è¯„ä¼°")
    else:
        sample_size = None
        print(f"ä½¿ç”¨å…¨æ ·æœ¬è¯„ä¼°ï¼ˆæ‰€æœ‰FAQï¼‰")
    
    print(f"\nå¼€å§‹è¯„ä¼°...")
    print(f"âš ï¸  æ³¨æ„ï¼šå…¨æ ·æœ¬è¯„ä¼°å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’Œæ›´å¤šAPIè°ƒç”¨")
    
    # è¯„ä¼°æ¯ä¸ªæ–‡ä»¶
    all_results = []
    
    for json_file in sorted(json_files):
        try:
            result = evaluate_faqs_from_json(json_file, method=method, sample_size=sample_size)
            result['file'] = json_file
            all_results.append(result)
        except Exception as e:
            print(f"âŒ å¤„ç† {json_file} æ—¶å‡ºé”™: {str(e)}")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æ€»ä½“è¯„ä¼°æ€»ç»“")
    print("="*70)
    
    for result in all_results:
        filename = Path(result['file']).stem
        print(f"\n{filename}:")
        print(f"  å¹³å‡ä¸€è‡´æ€§: {result['average_consistency']:.3f}")
        print(f"  ä¸€è‡´æ€§æ¯”ä¾‹: {result['consistency_rate']:.1%}")
        print(f"  ä¸€è‡´FAQ: {result['consistent_count']}/{result['total_count']}")
    
    print("\n" + "="*70)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*70)
    print("\nğŸ’¡ æç¤º:")
    print("1. æŸ¥çœ‹è¯¦ç»†çš„è¯„ä¼°ç»“æœæ–‡ä»¶: output/*_evaluation.json")
    print("2. ä¸€è‡´æ€§åˆ†æ•°>0.7çš„FAQè¢«è®¤ä¸ºæ˜¯é«˜è´¨é‡çš„")
    print("3. å¯ä»¥ç»“åˆæ‰‹åŠ¨éªŒè¯æ¥ç¡®è®¤ç»“æœ")


if __name__ == "__main__":
    main()

